#!/usr/bin/env python
# -*- coding: utf-8 -*-

import streamlit as st
import pandas as pd
import io
import re
from collections import defaultdict

# --- Streamlitãƒšãƒ¼ã‚¸ã®åˆæœŸè¨­å®š ---
st.set_page_config(page_title="æç›Šè¨ˆç®—æ›¸ãƒ‡ãƒ¼ã‚¿æ•´ç†ãƒ»åˆ†æãƒ„ãƒ¼ãƒ«", layout="wide")


def extract_data_from_chunk(df_chunk):
    """
    å˜ä¸€ã®è¡¨ãƒ–ãƒ­ãƒƒã‚¯(DataFrame)ã‚’å—ã‘å–ã‚Šã€3ã¤ã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã™ã‚‹ã€‚
    1. å¹´å·ã¯æœ€åˆã«å‡ºç¾ã—ãŸã‚‚ã®ã ã‘ã‚’æ¡ç”¨ã€‚
    2. ã€Œãã®ä»–ã€ã¯ä½ç½®ã‚’ä¿æŒã—ã¦è¤‡æ•°å‡ºç¾ã‚’è¨±å®¹ã€‚
    3. é …ç›®ã¯å‡ºç¾é †ã‚’ä¿æŒã€‚
    """
    if df_chunk.empty:
        return None, []

    year_pat = re.compile(r"^\s*20\d{2}\s*$")
    year_cells = []
    for r in range(df_chunk.shape[0]):
        for c in range(df_chunk.shape[1]):
            cell_value = df_chunk.iat[r, c]
            if pd.notna(cell_value) and bool(year_pat.match(str(cell_value))):
                year_cells.append({"row": r, "col": c, "year": int(str(cell_value).strip())})

    if not year_cells:
        return None, []

    # è¡Œç•ªå·ã€åˆ—ç•ªå·ã®é †ã§ã‚½ãƒ¼ãƒˆã—ã€å‡¦ç†é †ã‚’ç¢ºå®š
    year_cells.sort(key=lambda x: (x['row'], x['col']))

    processed_years = set()
    
    # æœ€åˆã«Aåˆ—ã®å…¨é …ç›®ã‚’æŠ½å‡ºã—ã€é †åºã‚’ç¢ºå®š
    initial_items = df_chunk[0].astype(str).str.strip().dropna()
    initial_items = initial_items[initial_items != ""]
    
    # ã€Œãã®ä»–ã€ã«ãƒ¦ãƒ‹ãƒ¼ã‚¯IDã‚’ä»˜ä¸
    is_sonota = initial_items == 'ãã®ä»–'
    if is_sonota.any():
        sonota_counts = initial_items.groupby(initial_items).cumcount()
        initial_items.loc[is_sonota] = initial_items.loc[is_sonota] + '_temp_' + sonota_counts[is_sonota].astype(str)
    
    # é‡è¤‡ã‚’å‰Šé™¤ã—ã¤ã¤é †åºã‚’ä¿æŒ
    all_items_ordered = initial_items.drop_duplicates(keep='first').tolist()
    df_result = pd.DataFrame({'å…±é€šé …ç›®': all_items_ordered})
    
    for cell in year_cells:
        year = cell['year']
        # --- â‘  å¹´å·ã¯æœ€åˆã«å‡ºã¦ããŸã‚‚ã®ã®ã¿ä½¿ç”¨ ---
        if year in processed_years:
            continue
        processed_years.add(year)

        val_col = cell['col']
        
        # å¹´å·ãƒ˜ãƒƒãƒ€ãƒ¼ä»¥é™ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        temp_df = df_chunk.iloc[cell['row'] + 1:, [0, val_col]].copy()
        temp_df.columns = ["å…±é€šé …ç›®", year]
        
        temp_df["å…±é€šé …ç›®"] = temp_df["å…±é€šé …ç›®"].astype(str).str.strip()
        temp_df.dropna(subset=["å…±é€šé …ç›®"], inplace=True)
        temp_df = temp_df[temp_df["å…±é€šé …ç›®"] != ""]

        # --- â‘¡ "ãã®ä»–"ã®ä½ç½®ã‚’ä¿æŒ ---
        is_sonota = temp_df['å…±é€šé …ç›®'] == 'ãã®ä»–'
        if is_sonota.any():
            sonota_counts = temp_df.groupby('å…±é€šé …ç›®').cumcount()
            temp_df.loc[is_sonota, 'å…±é€šé …ç›®'] = temp_df.loc[is_sonota, 'å…±é€šé …ç›®'] + '_temp_' + sonota_counts[is_sonota].astype(str)
        
        # é‡‘é¡ã‚’æ•°å€¤åŒ–
        temp_df[year] = pd.to_numeric(temp_df[year].astype(str).str.replace(",", ""), errors='coerce').fillna(0)
        
        # é‡è¤‡é …ç›®ã¯æœ€åˆã«å‡ºã¦ããŸã‚‚ã®ã‚’æ¡ç”¨
        temp_df = temp_df.drop_duplicates(subset=['å…±é€šé …ç›®'], keep='first')
        
        # çµæœã®DataFrameã«ãƒãƒ¼ã‚¸
        df_result = pd.merge(df_result, temp_df, on='å…±é€šé …ç›®', how='left')

    return df_result, all_items_ordered


def calculate_yoy(df):
    """å‰å¹´æ¯”ã¨å¢—æ¸›é¡ã‚’è¨ˆç®—ã™ã‚‹"""
    df_yoy = df.set_index("å…±é€šé …ç›®")
    
    # ã€Œãã®ä»–ã€ã‚’é›†è¨ˆã—ã¦ã‹ã‚‰è¨ˆç®—
    df_yoy.index = df_yoy.index.str.replace(r'_temp_\d+$', '', regex=True)
    df_yoy = df_yoy.groupby(df_yoy.index, sort=False).sum()
    
    df_diff = df_yoy.diff(axis=1)
    df_diff.columns = [f"{col} å¢—æ¸›é¡" for col in df_diff.columns]
    
    df_pct = df_yoy.pct_change(axis=1) * 100
    df_pct.columns = [f"{col} å¢—æ¸›ç‡(%)" for col in df_pct.columns]

    df_merged = pd.concat([df_yoy, df_diff, df_pct], axis=1)
    
    sorted_cols = []
    year_cols = sorted([col for col in df_yoy.columns if isinstance(col, int)])
    for year in year_cols:
        sorted_cols.append(year)
        if f"{year} å¢—æ¸›é¡" in df_merged.columns:
            sorted_cols.append(f"{year} å¢—æ¸›é¡")
        if f"{year} å¢—æ¸›ç‡(%)" in df_merged.columns:
            sorted_cols.append(f"{year} å¢—æ¸›ç‡(%)")
            
    df_merged = df_merged[sorted_cols].reset_index()
    return df_merged


def process_files_and_tables(excel_file):
    try:
        xls = pd.ExcelFile(excel_file)
        sheet_name_to_read = "æŠ½å‡ºçµæœ" if "æŠ½å‡ºçµæœ" in xls.sheet_names else xls.sheet_names[0]
        df_full = pd.read_excel(xls, sheet_name=sheet_name_to_read, header=None)
        st.info(f"ã‚·ãƒ¼ãƒˆã€Œ{sheet_name_to_read}ã€ã‚’å‡¦ç†ã—ã¦ã„ã¾ã™...")
    except Exception as e:
        st.error(f"Excelãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return None

    df_full[0] = df_full[0].astype(str)
    
    file_indices = df_full[df_full[0].str.contains(r'ãƒ•ã‚¡ã‚¤ãƒ«å:', na=False)].index.tolist()
    file_chunks = []
    if not file_indices:
        file_chunks.append(df_full)
    else:
        for i in range(len(file_indices)):
            start_idx = file_indices[i]
            end_idx = file_indices[i+1] if i + 1 < len(file_indices) else len(df_full)
            file_chunks.append(df_full.iloc[start_idx:end_idx].reset_index(drop=True))

    grouped_tables = defaultdict(list)
    master_item_order = defaultdict(list)

    for file_chunk in file_chunks:
        page_indices = file_chunk[file_chunk[0].str.contains(r'--- ãƒšãƒ¼ã‚¸', na=False)].index.tolist()
        
        table_chunks = []
        last_idx = 0
        for idx in page_indices:
            chunk = file_chunk.iloc[last_idx:idx]
            if not chunk.empty:
                table_chunks.append(chunk)
            last_idx = idx
        final_chunk = file_chunk.iloc[last_idx:]
        if not final_chunk.empty:
            table_chunks.append(final_chunk)

        for i, table_chunk in enumerate(table_chunks):
            processed_df, item_order = extract_data_from_chunk(table_chunk.reset_index(drop=True))
            if processed_df is not None and not processed_df.empty:
                grouped_tables[i].append(processed_df)
                # --- â‘¢ é …ç›®ã¯å‡ºã¦ããŸé †ã«ä¸Šã‹ã‚‰è¨˜è¼‰ ---
                current_order = master_item_order[i]
                for item in item_order:
                    if item not in current_order:
                        current_order.append(item)

    final_summaries = []
    for table_index in sorted(grouped_tables.keys()):
        list_of_dfs = grouped_tables[table_index]
        ordered_items = master_item_order[table_index]
        if not list_of_dfs: continue

        # çµ±åˆå…ˆã®ãƒ™ãƒ¼ã‚¹ã¨ãªã‚‹DataFrameã‚’ã€ãƒã‚¹ã‚¿ãƒ¼ã®é …ç›®é †ã§ä½œæˆ
        result_df = pd.DataFrame({'å…±é€šé …ç›®': ordered_items})

        # å„ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æŠ½å‡ºã—ãŸDataFrameã‚’é †ç•ªã«ãƒãƒ¼ã‚¸
        for df_to_merge in list_of_dfs:
            # çµåˆå‰ã«ã€result_dfã«æ—¢ã«å­˜åœ¨ã™ã‚‹åˆ—ã‚’df_to_mergeã‹ã‚‰å‰Šé™¤ï¼ˆå…ˆå‹ã¡ãƒ«ãƒ¼ãƒ«ï¼‰
            cols_to_drop = [col for col in df_to_merge.columns if col in result_df.columns and col != 'å…±é€šé …ç›®']
            df_filtered = df_to_merge.drop(columns=cols_to_drop)
            result_df = pd.merge(result_df, df_filtered, on='å…±é€šé …ç›®', how='left')

        result_df.fillna(0, inplace=True)
        
        # é …ç›®åˆ—ä»¥å¤–ã®åˆ—åï¼ˆå¹´åº¦ï¼‰ã‚’å–å¾—
        year_cols = [col for col in result_df.columns if col != 'å…±é€šé …ç›®']
        # å¹´åº¦åˆ—ã‚’æ•°å€¤ã«å¤‰æ›ã—ã¦ã‚½ãƒ¼ãƒˆ
        sorted_year_cols = sorted([col for col in year_cols if isinstance(col, (int, float)) or str(col).isdigit()], key=int)
        
        final_cols = ['å…±é€šé …ç›®'] + sorted_year_cols
        result_df = result_df[final_cols]

        for col in sorted_year_cols:
            result_df[col] = result_df[col].astype(int)
        
        # ã€Œãã®ä»–ã€ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯IDã‚’å‰Šé™¤
        result_df['å…±é€šé …ç›®'] = result_df['å…±é€šé …ç›®'].str.replace(r'_temp_\d+$', '', regex=True)

        final_summaries.append(result_df)
            
    return final_summaries


# --- Streamlitã®UIéƒ¨åˆ† ---
st.title("ï¿½ æç›Šè¨ˆç®—æ›¸ çµ±åˆãƒ‡ãƒ¼ã‚¿ä½œæˆãƒ„ãƒ¼ãƒ«ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ»ãƒšãƒ¼ã‚¸åˆ¥ï¼‰")
st.write("""
`ãƒ•ã‚¡ã‚¤ãƒ«å:` ã§åŒºåˆ‡ã‚‰ã‚ŒãŸå„ãƒ‡ãƒ¼ã‚¿å†…ã«ã‚ã‚‹ã€åŒã˜é †ç•ªã®è¡¨ï¼ˆ`--- ãƒšãƒ¼ã‚¸`åŒºåˆ‡ã‚Šï¼‰ã‚’ãã‚Œãã‚Œé›†è¨ˆã—ã€çµ±åˆã—ãŸã€Œã¾ã¨ã‚è¡¨ã€ã‚’ä½œæˆã—ã¾ã™ã€‚
""")

uploaded_file = st.file_uploader("å‡¦ç†ã—ãŸã„Excelãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ.xlsxï¼‰ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„", type=["xlsx"])

if uploaded_file:
    st.info(f"ãƒ•ã‚¡ã‚¤ãƒ«å: `{uploaded_file.name}`")

    if st.button("çµ±åˆã¾ã¨ã‚è¡¨ã‚’ä½œæˆ â–¶ï¸", type="primary"):
        with st.spinner("ãƒ‡ãƒ¼ã‚¿ã‚’æ•´ç†ãƒ»åˆ†æä¸­..."):
            all_summaries = process_files_and_tables(uploaded_file)

        if all_summaries:
            st.success(f"âœ… {len(all_summaries)}å€‹ã®çµ±åˆã¾ã¨ã‚è¡¨ãŒä½œæˆã•ã‚Œã¾ã—ãŸï¼")

            output_excel = io.BytesIO()
            with pd.ExcelWriter(output_excel, engine="xlsxwriter") as writer:
                for i, summary_df in enumerate(all_summaries):
                    summary_df.to_excel(writer, sheet_name=f"çµ±åˆã¾ã¨ã‚è¡¨_{i+1}", index=False)
            
            st.download_button(
                label="ğŸ“¥ å…¨ã¦ã®çµ±åˆã¾ã¨ã‚è¡¨ã‚’Excelã§ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=output_excel.getvalue(),
                file_name=f"çµ±åˆã¾ã¨ã‚è¡¨_{uploaded_file.name}",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            )
            st.divider()

            for i, summary_df in enumerate(all_summaries):
                with st.expander(f"â–¼ **çµ±åˆã¾ã¨ã‚è¡¨ {i+1}** ã®åˆ†æçµæœã‚’è¦‹ã‚‹"):
                    tab1, tab2, tab3 = st.tabs(["æ•´ç†å¾Œãƒ‡ãƒ¼ã‚¿", "ğŸ“ˆ æ¨ç§»ã‚°ãƒ©ãƒ•", "ğŸ†š å‰å¹´æ¯”ãƒ»å¢—æ¸›"])
                    
                    with tab1:
                        st.dataframe(summary_df)
                    
                    with tab2:
                        st.subheader("ä¸»è¦é …ç›®ã®å¹´åº¦æ¨ç§»ã‚°ãƒ©ãƒ•")
                        # ã‚°ãƒ©ãƒ•ç”¨ã«ã€Œãã®ä»–ã€ã‚’ä¸€æ—¦é›†è¨ˆ
                        df_for_chart = summary_df.copy()
                        df_for_chart['å…±é€šé …ç›®'] = df_for_chart['å…±é€šé …ç›®'].str.replace(r'_temp_\d+$', '', regex=True)
                        df_for_chart = df_for_chart.groupby('å…±é€šé …ç›®', sort=False).sum()
                        
                        items = df_for_chart.index.tolist()
                        default_items = [item for item in ["å£²ä¸Šé«˜", "å–¶æ¥­åˆ©ç›Š", "çµŒå¸¸åˆ©ç›Š", "å½“æœŸç´”åˆ©ç›Š"] if item in items]
                        selected_items = st.multiselect(
                            "ã‚°ãƒ©ãƒ•ã«è¡¨ç¤ºã™ã‚‹é …ç›®ã‚’é¸æŠ", options=items, default=default_items, key=f"chart_{i}"
                        )
                        if selected_items:
                            st.line_chart(df_for_chart.loc[selected_items].T)
                    
                    with tab3:
                        st.subheader("å‰å¹´æ¯”ãƒ»å¢—æ¸›é¡")
                        df_yoy_result = calculate_yoy(summary_df)
                        st.dataframe(df_yoy_result.style.format(precision=2, na_rep='-'))
        else:
            st.error("æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ•ã‚¡ã‚¤ãƒ«ã®å½¢å¼ã‚’ã”ç¢ºèªãã ã•ã„ã€‚")
else:
    st.warning("â˜ï¸ ä¸Šã®ãƒœã‚¿ãƒ³ã‹ã‚‰Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
